seed: 0 # Seed for numpy, torch and the game
max_num_gpus: null  # Fix the maximum number of GPUs to use. It's usually faster to use a single GPU (set it to 1) if it has enough memory. None will use every GPUs available

### Game
one_dim: 3 # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)
two_dim: 3
three_dim: 3
action_space: 12 # Fixed list of all possible actions. You should only edit the leng
players: 2 # List of players. You should only edit the length
stacked_observations: 0 # Number of previous observations and previous actions to add to the current observation

# Evaluate
muzero_player: 0 # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)
opponent: self  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn't influence training. None, "self", "random" or "expert" if implemented in the Game class


# Self-play
num_workers: 4 # Number of simultaneous threads/workers self-playing to feed the replay buffer
max_moves: 9 # Maximum number of moves if game is not finished before
num_simulations: 25  # Number of future moves self-simulated
discount: 1  # Chronological discount of the reward
temperature_threshold: null # Number of moves before dropping the temperature given by visit_softmax_temperature_fn to 0 (ie selecting the best action). If None, visit_softmax_temperature_fn is used every time

# Root prior exploration noise
root_dirichlet_alpha: 0.2 # Root prior exploration noise
root_exploration_fraction: 0.25

# UCB Formula
pb_c_base: 19652
pb_c_init: 1.25


### Network
support_size: 1 # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size. Choose it so that support_size <:  sqrt(max(abs(discounted reward)))
network: resnet  # neuralnetwork - "resnet" / "fullyconnected" 

# Residual Network
downsample: False # Downsample observations before representation network, False / "CNN" (lighter) / "resnet" (See paper appendix Network Architecture)
blocks: 1  # Number of blocks in the ResNet
channels: 16  # Number of channels in the ResNet
reduced_channels_reward: 16  # Number of channels in reward head
reduced_channels_value: 16  # Number of channels in value head
reduced_channels_policy: 16  # Number of channels in policy head
resnet_fc_reward_layers: [8] # Define the hidden layers in the reward head of the dynamic network
resnet_fc_value_layers: [8] # Define the hidden layers in the value head of the prediction network
resnet_fc_policy_layers: [8] # Define the hidden layers in the policy head of the prediction network

# Fully Connected Network
encoding_size: 32
fc_representation_layers: []  # Define the hidden layers in the representation network
fc_dynamics_layers: [16, 16]  # Define the hidden layers in the dynamics network 
fc_reward_layers: [8]  # Define the hidden layers in the reward network
fc_value_layers: [8]  # Define the hidden layers in the value network
fc_policy_layers: [16]  # Define the hidden layers in the policy network


# Training
training_steps: 500000 # Total number of training steps (ie weights update according to a batch)
batch_size: 256  # Number of parts of games to train on at each training step
checkpoint_interval: 10 # Number of training steps before using the model for self-playing
value_loss_weight: 0.25 # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)
train_on_gpu: auto  # Set "auto" to train on GPU if available

optimizer: Adam  # "Adam", "AdamW" or "SGD". Paper uses SGD
weight_decay: 0.0001  # L2 weights regularization
momentum: 0.9  # Used only if optimizer is SGD

 # Exponential learning rate schedule
lr_init: 0.003  # Initial learning rate 0001
lr_decay_rate: 1  # Set it to 1 to use a constant learning rate
lr_decay_steps: 10000

### Replay Buffer
replay_buffer_size: 3000  # Number of self-play games to keep in the replay buffer
num_unroll_steps: 3  # Number of game moves to keep for every batch element
td_steps: 9  # Number of steps in the future to take into account for calculating the target value
PER: False  # Prioritized Replay (See paper appendix Training), select in priority the elements in the replay buffer which are unexpected for the network
PER_alpha: 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1

# Reanalyze (See paper appendix Reanalyse)
use_last_model_value: True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)
selfplay_on_gpu: False
# num_reanalyse_workers: 1
# value_target_update_freq: 1  # Update frequency of the target model used to provide fresher value (and possibly policy) estimates
# use_updated_mcts_value_targets: False  # If True, root values targets are updated according to the re-execution of the MCTS (in this case, lagging parameters are used to run the MCTS to stabilize bootstrapping). Otherwise, a lagging value of the network (representation & value) is used to obtain the updated value targets.

# Adjust the self play / training ratio to avoid over/underfitting 
self_play_delay: 0  # Number of seconds to wait after each played game
training_delay: 0  # Number of seconds to wait after each training step
ratio: null  # Desired training steps per self played step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it
